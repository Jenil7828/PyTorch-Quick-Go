{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# PyTorch Tensors Basics Tutorial\n",
    "\n",
    "This tutorial covers the fundamental concepts of PyTorch tensors, including:\n",
    "- Installation and importing\n",
    "- GPU availability checking\n",
    "- Tensor creation methods\n",
    "- Data types and conversions\n",
    "- Mathematical operations\n",
    "- In-place operations\n",
    "- GPU training basics\n",
    "- Dimension manipulation\n",
    "- NumPy interoperability\n",
    "\n",
    "**Tensors** are specialized multi-dimensional arrays designed for mathematical and computational efficiency in deep learning.\n",
    "\n",
    "## Real World Examples of Tensors:\n",
    "- **0-Dimensional Tensor (Scalars/Rank 0)**: Represents a single value often used for simple constants. E.g. 5.0\n",
    "- **1-Dimensional Tensor (Vectors/Rank 1)**: Represents a sequence or a collection of values. E.g. Feature Vector in NLP for each word is represented as a 1D vector using embeddings [0.12, -0.84, 0.33]\n",
    "- **2-Dimensional Tensor (Matrix/Rank 2)**: Represents Tabular or Grid like data. E.g. A grayscale image [[0, 255, 128],[34, 90, 180]]\n",
    "- **3-Dimensional Tensor (Coloured Images/Rank 3)**: Adds a 3rd Dimension, often used for stacking data. E.g. RGB images\n",
    "- **4-Dimensional Tensor (Batches of RGB Images/Rank 4)**: Add the batch size as an additional dimension to 3D data. E.g.(batch size, width, height, channel)\n",
    "- **5-Dimensional Tensor (Video Data/Rank 5)**: Adds a time dimension for data that changes over time. E.g. Video Frames (batch size, width, height, channel, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "## 1. Installation and Importing\n",
    "\n",
    "First, let's install PyTorch if needed and import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-pytorch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (uncomment if needed)\n",
    "# !pip install torch\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-check",
   "metadata": {},
   "source": [
    "## 2. Checking GPU Availability\n",
    "\n",
    "It's important to check if CUDA GPU is available for faster computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available. Using CPU.\")\n",
    "\n",
    "print(\"Current device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensor-creation",
   "metadata": {},
   "source": [
    "## 3. Tensor Creation from Data\n",
    "\n",
    "There are many ways to create tensors in PyTorch. Let's explore the most common methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-from-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors from lists\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(\"Tensor from data:\")\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-special-tensors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors with specific shapes and values\n",
    "\n",
    "# Empty tensor (uninitialized data)\n",
    "x_empty = torch.empty(3, 4)\n",
    "print(\"Empty tensor:\")\n",
    "print(x_empty)\n",
    "\n",
    "# Zeros tensor\n",
    "x_zeros = torch.zeros(3, 4)\n",
    "print(\"\\nZeros tensor:\")\n",
    "print(x_zeros)\n",
    "\n",
    "# Ones tensor\n",
    "x_ones = torch.ones(3, 4)\n",
    "print(\"\\nOnes tensor:\")\n",
    "print(x_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-random-tensors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random tensors\n",
    "\n",
    "# Random tensor (uniform distribution 0-1)\n",
    "x_random = torch.rand(3, 4)\n",
    "print(\"Random tensor:\")\n",
    "print(x_random)\n",
    "\n",
    "# Random normal distribution\n",
    "x_randn = torch.randn(3, 4)\n",
    "print(\"\\nRandom normal tensor:\")\n",
    "print(x_randn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-structured-tensors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured tensors\n",
    "\n",
    "# Identity matrix\n",
    "x_eye = torch.eye(3)\n",
    "print(\"Identity tensor:\")\n",
    "print(x_eye)\n",
    "\n",
    "# Tensor with range\n",
    "x_range = torch.arange(0, 10, 2)\n",
    "print(\"\\nRange tensor:\")\n",
    "print(x_range)\n",
    "\n",
    "# Linspace tensor\n",
    "x_linspace = torch.linspace(0, 1, 5)\n",
    "print(\"\\nLinspace tensor:\")\n",
    "print(x_linspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensor-from-existing",
   "metadata": {},
   "source": [
    "## 4. Tensor Creation from Existing Tensors\n",
    "\n",
    "You can create new tensors based on existing ones while preserving their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-from-existing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original tensor\n",
    "original = torch.rand(3, 4)\n",
    "print(\"Original tensor:\")\n",
    "print(original)\n",
    "\n",
    "# Create new tensors with same shape but different content\n",
    "x_zeros_like = torch.zeros_like(original)\n",
    "print(\"\\nZeros like original:\")\n",
    "print(x_zeros_like)\n",
    "\n",
    "x_ones_like = torch.ones_like(original)\n",
    "print(\"\\nOnes like original:\")\n",
    "print(x_ones_like)\n",
    "\n",
    "x_rand_like = torch.rand_like(original)\n",
    "print(\"\\nRandom like original:\")\n",
    "print(x_rand_like)\n",
    "\n",
    "# Override data type (using float16 instead of int32 for rand_like)\n",
    "x_rand_like_float16 = torch.rand_like(original, dtype=torch.float16)\n",
    "print(\"\\nRandom like original (float16):\")\n",
    "print(x_rand_like_float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensor-properties",
   "metadata": {},
   "source": [
    "## 5. Tensor Properties\n",
    "\n",
    "Every tensor has several important properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensor-properties-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "print(\"Tensor properties:\")\n",
    "print(\"Tensor:\", tensor)\n",
    "print(\"Shape:\", tensor.shape)\n",
    "print(\"Size:\", tensor.size())\n",
    "print(\"Data type:\", tensor.dtype)\n",
    "print(\"Device:\", tensor.device)\n",
    "print(\"Number of dimensions:\", tensor.ndim)\n",
    "print(\"Number of elements:\", tensor.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-types",
   "metadata": {},
   "source": [
    "## 6. Data Types and Conversion\n",
    "\n",
    "PyTorch supports various data types, and you can convert between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-types-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different data types\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "double_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
    "\n",
    "print(\"Data types:\")\n",
    "print(\"Integer tensor:\", int_tensor, \"dtype:\", int_tensor.dtype)\n",
    "print(\"Float tensor:\", float_tensor, \"dtype:\", float_tensor.dtype)\n",
    "print(\"Double tensor:\", double_tensor, \"dtype:\", double_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "type-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type conversion\n",
    "float_to_int = float_tensor.to(torch.int32)\n",
    "print(\"\\nType conversion (to method):\")\n",
    "print(\"Float to int:\", float_to_int, \"dtype:\", float_to_int.dtype)\n",
    "\n",
    "# Alternative type conversion methods\n",
    "float_to_int_alt = float_tensor.int()\n",
    "print(\"Float to int (int method):\", float_to_int_alt, \"dtype:\", float_to_int_alt.dtype)\n",
    "\n",
    "int_to_float = int_tensor.float()\n",
    "print(\"Int to float:\", int_to_float, \"dtype:\", int_to_float.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "math-operations",
   "metadata": {},
   "source": [
    "## 7. Mathematical Operations\n",
    "\n",
    "PyTorch provides extensive support for mathematical operations on tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "element-wise-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "y = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "print(\"Mathematical Operations:\")\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "\n",
    "# Addition\n",
    "add_result1 = torch.add(x, y)\n",
    "add_result2 = x + y\n",
    "print(\"\\nAddition:\")\n",
    "print(\"torch.add(x, y):\", add_result1)\n",
    "print(\"x + y:\", add_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtraction\n",
    "sub_result1 = torch.sub(x, y)\n",
    "sub_result2 = x - y\n",
    "print(\"Subtraction:\")\n",
    "print(\"torch.sub(x, y):\", sub_result1)\n",
    "print(\"x - y:\", sub_result2)\n",
    "\n",
    "# Multiplication (element-wise)\n",
    "mul_result1 = torch.mul(x, y)\n",
    "mul_result2 = x * y\n",
    "print(\"\\nElement-wise multiplication:\")\n",
    "print(\"torch.mul(x, y):\", mul_result1)\n",
    "print(\"x * y:\", mul_result2)\n",
    "\n",
    "# Division\n",
    "div_result1 = torch.div(x, y)\n",
    "div_result2 = x / y\n",
    "print(\"\\nDivision:\")\n",
    "print(\"torch.div(x, y):\", div_result1)\n",
    "print(\"x / y:\", div_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "power-matmul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power\n",
    "pow_result1 = torch.pow(x, 2)\n",
    "pow_result2 = x ** 2\n",
    "print(\"Power (square):\")\n",
    "print(\"torch.pow(x, 2):\", pow_result1)\n",
    "print(\"x ** 2:\", pow_result2)\n",
    "\n",
    "# Matrix multiplication\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(3, 4)\n",
    "matmul_result1 = torch.matmul(a, b)\n",
    "matmul_result2 = a @ b\n",
    "print(\"\\nMatrix multiplication:\")\n",
    "print(\"a shape:\", a.shape, \"b shape:\", b.shape)\n",
    "print(\"torch.matmul(a, b) shape:\", matmul_result1.shape)\n",
    "print(\"a @ b shape:\", matmul_result2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scalar-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar operations\n",
    "scalar = 5\n",
    "x_scalar = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "print(\"Scalar operations:\")\n",
    "print(\"Original tensor:\", x_scalar)\n",
    "print(\"Add scalar:\", x_scalar + scalar)\n",
    "print(\"Multiply by scalar:\", x_scalar * scalar)\n",
    "print(\"Divide by scalar:\", x_scalar / scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inplace-ops",
   "metadata": {},
   "source": [
    "## 8. In-place Operations\n",
    "\n",
    "In-place operations modify tensors directly, saving memory but changing the original tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inplace-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In-place operations:\")\n",
    "x_inplace = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "y_inplace = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "print(\"Before in-place operations:\")\n",
    "print(\"x:\", x_inplace)\n",
    "print(\"y:\", y_inplace)\n",
    "\n",
    "# In-place addition (modifies x)\n",
    "x_inplace.add_(y_inplace)\n",
    "print(\"\\nAfter x.add_(y):\")\n",
    "print(\"x:\", x_inplace)\n",
    "\n",
    "# In-place subtraction\n",
    "x_inplace.sub_(2)\n",
    "print(\"\\nAfter x.sub_(2):\")\n",
    "print(\"x:\", x_inplace)\n",
    "\n",
    "# In-place multiplication\n",
    "x_inplace.mul_(2)\n",
    "print(\"\\nAfter x.mul_(2):\")\n",
    "print(\"x:\", x_inplace)\n",
    "\n",
    "print(\"\\nNote: In-place operations save memory but modify the original tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu-operations",
   "metadata": {},
   "source": [
    "## 9. Training on GPU (GPU Operations)\n",
    "\n",
    "GPU operations can significantly speed up computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Operations:\")\n",
    "if torch.cuda.is_available():\n",
    "    # Move tensors to GPU\n",
    "    x_gpu = torch.randn(3, 4).to(device)\n",
    "    y_gpu = torch.randn(3, 4).to(device)\n",
    "    \n",
    "    print(\"Tensors moved to GPU:\")\n",
    "    print(\"x_gpu device:\", x_gpu.device)\n",
    "    print(\"y_gpu device:\", y_gpu.device)\n",
    "    \n",
    "    # Operations on GPU\n",
    "    result_gpu = x_gpu + y_gpu\n",
    "    print(\"GPU operation result device:\", result_gpu.device)\n",
    "    \n",
    "    # Move back to CPU for printing\n",
    "    result_cpu = result_gpu.cpu()\n",
    "    print(\"Result moved back to CPU:\", result_cpu.device)\n",
    "    \n",
    "    # Direct GPU tensor creation\n",
    "    gpu_tensor = torch.randn(2, 3, device=device)\n",
    "    print(\"Direct GPU tensor device:\", gpu_tensor.device)\n",
    "else:\n",
    "    print(\"GPU not available, skipping GPU operations\")\n",
    "    # CPU operations as fallback\n",
    "    x_cpu = torch.randn(3, 4)\n",
    "    y_cpu = torch.randn(3, 4)\n",
    "    result_cpu = x_cpu + y_cpu\n",
    "    print(\"CPU operation completed on device:\", result_cpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensions",
   "metadata": {},
   "source": [
    "## 10. Playing with Dimensions\n",
    "\n",
    "Dimension manipulation is crucial for neural network operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reshape-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimension operations:\")\n",
    "x_dim = torch.randn(2, 3, 4)\n",
    "print(\"Original tensor shape:\", x_dim.shape)\n",
    "\n",
    "# Reshape\n",
    "reshaped = x_dim.view(3, 8)\n",
    "print(\"Reshaped (view):\", reshaped.shape)\n",
    "\n",
    "# Another reshape method\n",
    "reshaped2 = x_dim.reshape(4, 6)\n",
    "print(\"Reshaped (reshape):\", reshaped2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "squeeze-unsqueeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze (remove dimensions of size 1)\n",
    "x_squeeze = torch.randn(1, 3, 1, 4, 1)\n",
    "print(\"Before squeeze:\", x_squeeze.shape)\n",
    "squeezed = x_squeeze.squeeze()\n",
    "print(\"After squeeze:\", squeezed.shape)\n",
    "\n",
    "# Unsqueeze (add dimensions of size 1)\n",
    "x_unsqueeze = torch.randn(3, 4)\n",
    "print(\"\\nBefore unsqueeze:\", x_unsqueeze.shape)\n",
    "unsqueezed = x_unsqueeze.unsqueeze(0)  # Add dimension at index 0\n",
    "print(\"After unsqueeze(0):\", unsqueezed.shape)\n",
    "unsqueezed2 = x_unsqueeze.unsqueeze(-1)  # Add dimension at the end\n",
    "print(\"After unsqueeze(-1):\", unsqueezed2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transpose-permute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose\n",
    "x_transpose = torch.randn(3, 4)\n",
    "print(\"Original shape:\", x_transpose.shape)\n",
    "transposed = x_transpose.t()  # 2D transpose\n",
    "print(\"Transposed shape:\", transposed.shape)\n",
    "\n",
    "# Permute (generalized transpose)\n",
    "x_permute = torch.randn(2, 3, 4)\n",
    "print(\"\\nBefore permute:\", x_permute.shape)\n",
    "permuted = x_permute.permute(2, 0, 1)  # Rearrange dimensions\n",
    "print(\"After permute(2, 0, 1):\", permuted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concat-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation\n",
    "x1 = torch.randn(2, 3)\n",
    "x2 = torch.randn(2, 3)\n",
    "print(\"Tensors to concatenate:\", x1.shape, x2.shape)\n",
    "\n",
    "concat_dim0 = torch.cat([x1, x2], dim=0)  # Concatenate along dimension 0\n",
    "print(\"Concatenated along dim 0:\", concat_dim0.shape)\n",
    "\n",
    "concat_dim1 = torch.cat([x1, x2], dim=1)  # Concatenate along dimension 1\n",
    "print(\"Concatenated along dim 1:\", concat_dim1.shape)\n",
    "\n",
    "# Stacking\n",
    "stacked = torch.stack([x1, x2], dim=0)  # Create new dimension\n",
    "print(\"Stacked shape:\", stacked.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numpy-interop",
   "metadata": {},
   "source": [
    "## 11. NumPy vs PyTorch Interoperability\n",
    "\n",
    "PyTorch tensors can easily convert to and from NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numpy-pytorch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NumPy vs PyTorch interoperability:\")\n",
    "\n",
    "# Note: NumPy might not be available in all environments\n",
    "try:\n",
    "    import numpy as np\n",
    "    \n",
    "    # NumPy array to PyTorch tensor\n",
    "    numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "    print(\"NumPy array:\")\n",
    "    print(numpy_array)\n",
    "    print(\"PyTorch tensor from NumPy:\")\n",
    "    print(tensor_from_numpy)\n",
    "    print(\"Tensor dtype:\", tensor_from_numpy.dtype)\n",
    "    \n",
    "    # PyTorch tensor to NumPy array\n",
    "    pytorch_tensor = torch.randn(2, 3)\n",
    "    numpy_from_tensor = pytorch_tensor.numpy()\n",
    "    print(\"\\nPyTorch tensor:\")\n",
    "    print(pytorch_tensor)\n",
    "    print(\"NumPy array from PyTorch:\")\n",
    "    print(numpy_from_tensor)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"NumPy not available. Skipping NumPy interoperability examples.\")\n",
    "    print(\"Install NumPy with: pip install numpy\")\n",
    "    \n",
    "    # Alternative: Create similar examples with pure PyTorch\n",
    "    print(\"Alternative: Converting between different tensor types\")\n",
    "    float_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "    int_tensor = float_tensor.to(torch.int32)\n",
    "    print(\"Float tensor:\", float_tensor)\n",
    "    print(\"Int tensor:\", int_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important note about memory sharing (if NumPy is available)\n",
    "try:\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"Memory sharing demonstration:\")\n",
    "    numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "    shared_tensor = torch.from_numpy(numpy_array)\n",
    "    print(\"Original NumPy array:\", numpy_array)\n",
    "    shared_tensor[0, 0] = 999  # Modify tensor\n",
    "    print(\"NumPy array after tensor modification:\", numpy_array)\n",
    "    print(\"They share memory!\")\n",
    "    \n",
    "    # To avoid memory sharing, use .clone()\n",
    "    numpy_array = np.array([[1, 2, 3], [4, 5, 6]])  # Reset array\n",
    "    independent_tensor = torch.from_numpy(numpy_array).clone()\n",
    "    independent_tensor[0, 1] = 888\n",
    "    print(\"\\nNumPy array after independent tensor modification:\", numpy_array)\n",
    "    print(\"Independent tensor:\", independent_tensor)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"NumPy not available for memory sharing demonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 12. Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Always check device compatibility** (CPU/GPU) before running computations\n",
    "2. **Be aware of tensor data types** for computation efficiency\n",
    "3. **Use appropriate tensor creation methods** for your use case\n",
    "4. **In-place operations save memory** but modify original tensors\n",
    "5. **GPU tensors require explicit device management**\n",
    "6. **Dimension operations are crucial** for neural network operations\n",
    "7. **NumPy interoperability enables seamless** data science workflows\n",
    "8. **Always consider memory implications** when working with large tensors\n",
    "\n",
    "### Next Steps:\n",
    "- Explore automatic differentiation with `torch.autograd`\n",
    "- Learn about neural network building blocks in `torch.nn`\n",
    "- Practice with datasets using `torch.utils.data`\n",
    "- Try optimization algorithms in `torch.optim`\n",
    "\n",
    "**Congratulations!** You've completed the PyTorch Tensors Basics tutorial. You now have a solid foundation for working with tensors in PyTorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}